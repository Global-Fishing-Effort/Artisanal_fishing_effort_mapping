---
title: "Prepare satelitte imagery data from Sentinel-2"
output: html_document
date: "2025-07-29"
---

## Summary

Here we prep data Sentinel-2 satellite imagery vessel detections from Global Fishing Watch (10-meter resolution) to use in our models. First, you will need to download the data from here: https://globalfishingwatch.org/data-download/datasets/public-sentinel2-vessel-detections%3Av1.0

Reference: 
Global Fishing Watch, 2025. 2019 (annual data) Vessel detections from Sentinel 2. https://doi.org/10.5281/ZENODO.15978309

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(here)
library(tidyverse)
library(terra)
library(dplyr)
library(geosphere)
library(sf)
library(rnaturalearth)
library(rnaturalearthdata)
library(qs)
library(stringr)
library(doParallel)

source(here("R/dir.R"))
```
 
Let's explore one file from July 2025, since other files are much larger and unwieldy: 

```{r}
# read in smallest data and take a look
vessels_2025_july <- read.csv(file.path(rdsi_raw_dir, "global_fishing_watch/sentinel_2_data/sentinel2_vessel_detections_pipev3_20250705.csv"))

colnames(vessels_2025_july)

#  [1] "detect_id"                "scene_id"                 "lat"                      "lon"                     
#  [5] "detect_timestamp"         "speed_kn_inferred"        "heading_deg_inferred"     "length_m_inferred"       
#  [9] "presence_score"           "nonvessel_score"          "cloud_score"              "likely_infrastructure"   
# [13] "potential_ice"            "matching_score"           "matching_score_secondary" "matching_confidence"     
# [17] "mmsi"                     "ais_lat_before"           "ais_lat_after"            "ais_lon_before"          
# [21] "ais_lon_after"            "ais_speed_kn_before"      "ais_speed_kn_after"       "ais_course_before"       
# [25] "ais_course_after"         "ais_timestamp_before"     "ais_timestamp_after"      "ais_length_m"     

## ok so i think we will want lon, lat, length_m_inferred, presence_score, nonvessel_score, likely_infrastructure, potential_ice

vessels_july_df <- vessels_2025_july %>%
  dplyr::select(lon, lat, length_m_inferred, presence_score, nonvessel_score, likely_infrastructure, potential_ice, matching_score, matching_confidence, mmsi) %>%
  filter(likely_infrastructure == "false", # remove any infrastructure
         potential_ice == "false", # remove any ice
         presence_score >= 0.5,
         nonvessel_score < 0.5) # only keep those with high presence score and those that are vessels

non_mmsi_keep <- vessels_july_df %>%
  filter(is.na(mmsi))

mmsi_remove <- vessels_july_df %>%
  filter(!is.na(mmsi), 
         matching_confidence >= 0.5) # remove any that have mmsi, assuming these are industrial vessels

mmsi_keep <- vessels_july_df %>%
  filter(!is.na(mmsi),
         matching_confidence < 0.5) # only keep mmsi if the matching confidence is low

vessels_df_fin <- rbind(non_mmsi_keep, mmsi_keep) %>%
  dplyr::select(lon, lat, length_m_inferred)

```

Now we apply the same filtering as above, but to all files. We will need a file for each year that is lon, lat, length_m_inferred, and year. Each row represents a vessel detection. 

```{r}
sentinel_files <- list.files(file.path(rdsi_raw_dir, "global_fishing_watch/sentinel_2_data"), full.names = TRUE, pattern = "csv")

cl <- makeCluster(13) # or however many cores you want
registerDoParallel(cl)

foreach(file = sentinel_files[2:13],
        .packages = c("dplyr", "stringr", "tools")) %dopar% {
  years <- str_extract(file, "(?<=_pipev3_)\\d{4}")
  
  vessels_raw <- read.csv(file)
  
  vessels_df <- vessels_raw %>%
    dplyr::select(lon, lat, length_m_inferred, presence_score, nonvessel_score,
                  likely_infrastructure, potential_ice, matching_score,
                  matching_confidence, mmsi) %>%
    filter(likely_infrastructure == "false",
           potential_ice == "false",
           presence_score >= 0.5,
           nonvessel_score < 0.5)
  
  non_mmsi_keep <- vessels_df %>%
    filter(is.na(mmsi))
  
  mmsi_keep <- vessels_df %>%
    filter(!is.na(mmsi),
           matching_confidence < 0.5)
  
  vessels_df_fin <- rbind(non_mmsi_keep, mmsi_keep) %>%
    dplyr::select(lon, lat, length_m_inferred)
  
  # Optionally, add the year as a column
  vessels_df_fin$year <- years
  
  out_name <- paste0("filtered_", tools::file_path_sans_ext(basename(file)), ".csv")
  write.csv(vessels_df_fin, file = file.path(rdsi_dir, "prep/sentinel_2_prepped/prepped_10m_res", out_name), row.names = FALSE) # save to your large file storage
  
}


stopCluster(cl)

```


Explore the data 

```{r}
# 2024 data
vessels_final <- read.csv(file.path(rdsi_dir, "prep/sentinel_2_prepped/prepped_10m_res/filtered_sentinel2_vessel_detections_pipev3_2024.csv"))

# 1. Round centroid coordinates to any resolution you prefer
cell_size <- 0.5 # You can change this to 1, 0.5, 0.01, 0.05, etc.

vessel_batch_counts <- vessels_final %>%
  mutate(
    lon_bin = floor(lon / cell_size) * cell_size,
    lat_bin = floor(lat / cell_size) * cell_size
  ) %>%
  group_by(lon_bin, lat_bin) %>%
  summarise(vessel_count = n(), .groups = "drop") %>%
  rast(., type = "xyz")

plot(vessel_batch_counts) # cool, looks like the whole globe. 


```


Brainstorming: 

How to determine flag country? 
 - we will be assuming that if flag country = eez area where the vessel is detected. So we will have to count the number of vessels detected in each eez and calculate a proportion in each cell.

