---
title: "Fit prediction model for fishing vessels: stage 1"
output: html_document
date: "2025-07-23"
editor_options: 
  chunk_output_type: console
---

# Summary 

We use the data compiled in the previous scripts to fit a random forest regression to predict the presence (1) or absence (0) of fishing vessels in every cell. We will do this for each flag country individually.  

## Environment

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

rm(list = ls())

library(tidyverse)
library("qs")
library(foreach)
library(doParallel)
library(here)
library(tictoc)
library(progress)
library(terra)
library(glue)
library(arrow)
library(strex)
library(broom)
library(PRROC)
library(ranger)

source(here("R/dir.R"))

```

## Functions to prepare enviro and effort data  

Read in model predictor data 

```{r}
elnino <- read.csv(here("data/model_features/enso_index.csv"))

pdo <- read.csv(here("data/model_features/pdo_index.csv")) 

world_bank <- read.csv(here("data/model_features/world_bank_regions.csv")) 

gfi_df <- read.csv(here("data/model_features/global_fishing_index_governance.csv"))

# Load prepared data
model_data <- readRDS(here("data/model_features/prepared_data_0.5_all_skylight_sentinel.rds")) %>%
  mutate(year = as.numeric(year)) %>%
  mutate(presence = 1) %>%
  filter(year < 2025)

## read in environmental variables

  global_grid <- read.csv(here("data/model_features/global_grid.csv"))

  ocean_data <- read.csv(here("data/model_features/sst_data.csv")) %>%
    left_join(qs::qread(here("data/model_features/chl_data.qs"))) %>%
    left_join(qs::qread(here("data/model_features/wind_data.qs"))) %>%
    dplyr::select(-pixel_area_m2)

  spatial_data <- global_grid %>%
    left_join(read.csv(here("data/model_features/gfw_static_spatial_measures.csv")) %>% dplyr::select(-lat, -lon), by = "pixel_id") %>%
   left_join(read.csv(here("data/model_features/mesopelagiczones/mesopelagiczones_fixed.csv")), by = "pixel_id") %>%
   left_join(read.csv(here("data/model_features/eez/eez_fix.csv")), by = "pixel_id") %>%
   left_join(read.csv(here("data/model_features/fao/fao_fixed.csv")), by = "pixel_id") %>%
   left_join(read.csv(here("data/model_features/ocean/ocean_fixed.csv")), by = "pixel_id") %>%
    left_join(read.csv(here("data/model_features/seamounts.csv")), by = "pixel_id") %>% 
   crossing(., year = c(2009:2024)) 
  
  
  env_data <- spatial_data %>%
    left_join(ocean_data) %>% # cool.
    dplyr::select(-geometry_wkt) %>%
    left_join(elnino) %>%
    left_join(pdo) %>%
    left_join(world_bank, by = c("eez_id")) %>%
    mutate(eez_region_world_bank_7 = ifelse(eez_sovereign %in% c("High seas", "Land"), "High seas", eez_region_world_bank_7)) %>% 
    left_join(gfi_df, by = c("eez_sovereign" = "flag_fin")) %>% # add in global fishing index data here
    mutate(gov_score = ifelse(eez_id >= 99999 & is.na(gov_score), "high_seas", gov_score)) %>%
        mutate(gov_score = ifelse(eez_id < 99999 & is.na(gov_score), "no_data", gov_score)) %>%
    dplyr::select(-eez_sovereign, -nearest_seamount_id) %>% 
    distinct() 
  
```

## Specify model formula

```{r}

# Prepare model formula
# apply model on train 
model_formula_rf <- formula(
  presence ~ 
    # Categorical/factor predictors
    length_category +
    meso_id +
    eez_id +
    fao_id +
    ocean +  # Spatial categorical variables
    eez_region_world_bank_7 + # world bank regions
    gov_score + # global fishing index; make sure this is categorical and not a numeric variable
    # Continuous predictors
    lon + lat + 
    elevation_m + # depth
    distance_from_port_m + 
    distance_from_shore_m +
    chl_mg_per_m3_mean + 
    chl_mg_per_m3_sd +
    sst_c_mean + 
    sst_c_sd +
    wind_speed_ms_mean + 
    wind_speed_ms_sd +
    enso_index_mean + # el nino data 
    enso_index_sd + 
    pdo_index_mean +  # pacific decadal oscillation
    pdo_index_sd + 
    nearest_seamount_distance_m + 
    year 
)
  

hist_fish_data <- qs::qread(here("data/model_features/rousseau_total_art_vessels.qs"))
flags <- unique(hist_fish_data$flag_fin) # get the flags we need to run models for # 164 of them

env_grid <- env_data %>% 
  dplyr::select(pixel_id, lon, lat) %>% distinct()

```

## Run models and variable selection for every individual country

 - calculate the full model using all variables available per flag country
 - calculate variable importance metrics and Area Under Curve (AUC)
 - Set a threshold of the 10th quantile of the variable importance (GINI index for classification) and remove any variables with variable importance less than that
 - Rerun model with new variables and calculate model importance metrics and AUC-PR again
 - If the AUC doesn't increase at all, we stop the model pruning, if it does, we continue, hoping that the AUC will improve even more in the next iteration. 
 - If the threshold doesn't remove any variables, we increase the threshold by 1% until a variable is removed, and rerun the process
 - We loop through this until the AUC does not improve at all. 
  - NOTE: We keep year as a variable no matter what, because we need to retain differences over time. 
 
 NOTE: lets use Area Under the Precision-Recall Curve (AUC-PR) instead of RMSE because RMSE isn't really relevant to classification regression; Higher AUC-PR value is better. 

Run full models first 
 -  With 100 trees it only takes ~40 mins

```{r}
# get flags to run
flags <- unique(hist_fish_data$flag_fin)

missing_flags <- setdiff(unique(hist_fish_data$flag_fin), unique(model_data$flag_fin)) # 0 good

flags <- setdiff(flags, missing_flags) #164 flags! 

# done_flags <- unique(str_after_last(str_before_last(list.files(file.path(rdsi_dir, "prep/random_forest/artisanal_skylight_sentinel/stage_1_models/pruning/")), "_"), "_"))

flags_to_run <- setdiff(flags, "") # add any flags we've already run

# Set up parallel backend
num_cores <- 20
cl <- makeCluster(num_cores)
registerDoParallel(cl)

# Run the loop in parallel
foreach(flag = flags_to_run, .packages = c("dplyr", "tidyr", "randomForest", "qs", "glue", "tictoc", "ranger")) %dopar% {
  
    # flag = "HKG"
  model_data_flag <- model_data %>%
    dplyr::select(pixel_id, lon, lat, flag_fin, length_category, year, presence) %>%
    filter(flag_fin == flag) # filter for specific flag from vessel count data
  
  distinct_cats <- model_data_flag %>%
    distinct(year, flag_fin, length_category)
  
  full_grid <- tidyr::crossing(env_grid, distinct_cats) %>%
    filter(!is.na(pixel_id))
  
  data_random_forest <- full_grid %>%
    left_join(env_data) %>%
    left_join(model_data_flag, by = c("pixel_id", "lon", "lat", "year", "flag_fin", "length_category")) %>%
    mutate(presence = ifelse(is.na(presence), 0, presence)) %>%
    dplyr::select(-pixel_id) %>%
        na.omit() %>%
    mutate(presence = as.factor(presence)) %>%
    distinct() # join with the environmental/spatial data because we need to run the model on ALL cells for classification regression
  
  if (nrow(data_random_forest) == 1) {
    cat(flag, "not enough rows\n")
    return(NULL)
  }

  # Split data into training and testing sets
  set.seed(123)
  samp <- sample(nrow(data_random_forest), 0.6 * nrow(data_random_forest)) 
  train <- data_random_forest[samp, ]
  test <- data_random_forest[-samp, ]
  
    tic()
  model <- ranger(
  formula = model_formula_rf,
  data = train,
  num.trees = 100, # run with 100 to start
  importance = "impurity",
  classification = TRUE,
  probability = TRUE,
  write.forest = TRUE
)
    toc() # for timings
    

 # model <- qs::qread(glue(file.path(rdsi_dir, "prep/random_forest/stage_1_models/pruning/stage_1_rf_train_{flag}_27.qs")))

      var_imp_i <- data.frame(importance(model))
  
  n_vars <- nrow(var_imp_i)
  qs::qsave(model, glue(file.path(rdsi_dir, "prep/random_forest/artisanal_skylight_sentinel/stage_1_models/pruning/stage_1_rf_train_{flag}_{n_vars}.qs"))) # save models
  
}

stopCluster(cl)

```
 
Now apply variable selection methods 
 - Takes ~4 hours? with 12 cores and 100 trees

```{r}
# Threshold-Based Selection: Remove variables with importance scores below a certain threshold (e.g., the median or a predefined percentage of the highest importance value).
#### How to check when model performance stabilizes?? Look at AUC-PR vs number of variables in each model. When the AUC-PR stops getting better is where you make the variable delineation.  

## lets try the threshold based selection, where the threshold is 10% of max importance score (i.e. we keep any variable that is above that 10% of max importance score) and rerun the model and test to see if AUC improves or not. BUT KEEP YEAR IN NO MATTER WHAT

# Set up parallel backend

## we should move CHN to the front of the list of flags. It takes the longest to model, so should be in the first batch of cores. 

flags <- unique(c("CHN", "RUS", "USA", flags)) # move large ones to the front of the list since they take the longest to run...

num_cores <- 12
cl <- makeCluster(num_cores)
registerDoParallel(cl)

# Run the loop in parallel
foreach(flag = flags, .packages = c("dplyr", "tidyr", "randomForest", "qs", "glue", "tictoc", "ranger", "PRROC")) %dopar% {

   # flag = "HKG"
    auc_values <- c()
    num_vars <- c()
  model_data_flag <- model_data %>%
    dplyr::select(lon, lat, flag_fin, length_category, year, presence) %>%
    filter(flag_fin == flag)
  
  if(nrow(model_data_flag) == 1){
    cat(flag, "not enough categories to model... skipping\n")
    next()
  }
  
  distinct_cats <- model_data_flag %>%
    distinct(year, flag_fin, length_category)
  
  full_grid <- tidyr::crossing(env_grid, distinct_cats)
  
  data_random_forest <- full_grid %>%
    left_join(env_data, by = c("lon", "lat", "year")) %>%
    left_join(model_data_flag, by = c("lon", "lat", "year", "flag_fin", "length_category")) %>%
    mutate(presence = ifelse(is.na(presence), 0, presence)) %>%
    na.omit() %>%
    mutate(presence = as.factor(presence)) %>%
    distinct()
  
  if (nrow(data_random_forest) == 1) {
    cat(flag, "not enough rows\n")
    next()
  }
  
  # Split data into training and testing sets
  set.seed(123)
  samp <- sample(nrow(data_random_forest), 0.6 * nrow(data_random_forest)) 
  train <- data_random_forest[samp, ]
  test <- data_random_forest[-samp, ]
  
  ## if the file doesn't exist for some reason, run this. They should all be there from the previous chunk though.
  if(!file.exists((glue(file.path(rdsi_dir, "prep/random_forest/artisanal_skylight_sentinel/stage_1_models/pruning/stage_1_rf_train_{flag}_24.qs"))))){
    
        
    tic()
  model <- ranger(
        formula = model_formula_rf,
        data = train,
        num.trees = 100, # run with 100 trees
        importance = "impurity",
        classification = TRUE,
        probability = TRUE,
        write.forest = TRUE
      )
    toc()
  
  }else{ 
    model <- qs::qread(glue(file.path(rdsi_dir, "prep/random_forest/artisanal_skylight_sentinel/stage_1_models/pruning/stage_1_rf_train_{flag}_24.qs")))
}
  
  var_imp_i <- data.frame(importance(model)) %>%
     rename("importance" = 1)
  num_vars <- c(nrow(var_imp_i))
  threshold_i <- quantile(var_imp_i[, "importance"], 0.10)  # Use the 10th percentile instead of a fixed multiplier

  if (threshold_i == 0) {
    threshold_i <- 0.0000001
  }
  
  prob <- as.data.frame(predict(model, data = test)$predictions) %>%
        mutate(presence = ifelse(`1` >= 0.5, 1, 0))
  pred_probs <- prob[, "presence"]
  
  true_labels <- as.numeric(as.character(test$presence))  # Replace with the actual labels in your test data

  pr_curve <- pr.curve(scores.class0 = pred_probs, weights.class0 = true_labels, curve = TRUE)
  auc_pr <- pr_curve$auc.integral # higher auc_pr is BETTER. So if it gets lower, then we don't want the next model 

  auc_values <- c(auc_values, auc_pr)
  
  iteration <- 1
 while (TRUE) {

   selected_vars <- var_imp_i %>% 
     filter(importance > threshold_i) %>%
      row.names()
   
    selected_vars <- unique(c("year", selected_vars)) # always retain year and the ais reception data (for ais data biases)
    
     if (length(selected_vars) < 6) break  # Ensure at least 5 variables remain
    
        # Increase the threshold iteratively until at least one variable is removed
    while (length(selected_vars) == num_vars[length(num_vars)]) {
        threshold_i <- threshold_i * 1.01  # Increase threshold by 1%
   
   selected_vars <- var_imp_i %>% 
     filter(importance > threshold_i) %>%
      row.names()
   
    selected_vars <- unique(c("year", selected_vars)) # always retain year and the ais reception data (for ais data biases)
   }

       n_vars <- length(selected_vars)
    file_path <- glue(file.path(rdsi_dir, "prep/random_forest/artisanal_skylight_sentinel/stage_1_models/pruning/stage_1_rf_train_{flag}_{n_vars}.qs"))
   
   if(!file.exists(file_path)){
   
     model_i <- ranger(
        formula = presence ~ .,
        data = train[, c("presence", selected_vars)],
        num.trees = 100,
        importance = "impurity",
        classification = TRUE,
        probability = TRUE,
        write.forest = TRUE
      )
     
   }else{
     
     model_i <- qs::qread(file_path)
   }
    
    var_imp_i <- data.frame(importance(model_i)) %>% 
      rename("importance" = 1)
    
    # Compute a new threshold dynamically based on the 10th percentile
      threshold_i <- quantile(var_imp_i[, 1], 0.10)  # Use the 10th percentile instead of a fixed multiplier

        prob_i <- as.data.frame(predict(model_i, data = test)$predictions) %>%
          mutate(presence = ifelse(`1` >= 0.5, 1, 0))
  pred_probs_i <- prob_i[, "presence"]

   true_labels_i <- as.numeric(as.character(test$presence))  # Replace with the actual labels in your test data

  pr_curve_i <- pr.curve(scores.class0 = pred_probs_i, weights.class0 = true_labels_i, curve = TRUE)
  auc_pr_i <- pr_curve_i$auc.integral # higher auc_pr is BETTER. So if it gets lower, then we don't want the next model 

  auc_values <- c(auc_values, auc_pr_i)
    
  num_vars <- c(num_vars, length(selected_vars))
    

    if(!file.exists(file_path)){

          qs::qsave(model_i, file_path)
     }
    
    if (length(auc_values) > 1) {
      
      model_improvement <- auc_values[length(auc_values)] - auc_values[length(auc_values) - 1]
        
        if (model_improvement <= 0)  break  # Stop if improvement is <=0
        
    }
    
    iteration <- iteration + 1
 }

}

# Stop parallel cluster
stopCluster(cl)


```


Now write code to select the "best" model per the variable selection from above. We will rerun the model with just those variables on the FULL dataset to leverage all of the data from skylight and sentinel and save. We will run these with 500 trees. 
 - Around 8 hours with 12 cores

```{r}
# grab the flags that were run in the folder
stage_1_path <- file.path(rdsi_dir, "prep/random_forest/artisanal_skylight_sentinel/stage_1_models/pruning")
stage_1_files <- list.files(stage_1_path, full.names = TRUE)
stage_1_flags <- unique(sub(".*stage_1_rf_train_([A-Z]{3})_.*", "\\1", stage_1_files))


stage_1_path_done <- file.path(rdsi_dir, "prep/random_forest/artisanal_skylight_sentinel/stage_1_models/")
stage_1_files_done <- list.files(stage_1_path_done, full.names = TRUE)[-1]
stage_1_flags_done <- unique(sub(".*stage_1_rf_model_full_data_([A-Z]{3})_.*", "\\1", stage_1_files_done))

stage_1_flags <- setdiff(stage_1_flags, stage_1_flags_done)


num_cores <- 12
cl <- makeCluster(num_cores)
registerDoParallel(cl)

foreach(flag = stage_1_flags, .packages = c("dplyr", "tidyr", "randomForest", "qs", "glue", "tictoc", "strex", "stats", "ranger", "PRROC")) %dopar% {

#  flag = "HKG"
  flag_files <- list.files(stage_1_path, pattern = glue("_{flag}_"))
  # select the model with the next to lowest number of variables. E.g., if the final model run for ZAF is 17, then we want to grab the model with the next to lowest number of variables, which is 19. 
  n_variables <- as.numeric(str_before_first(str_after_last(flag_files, "_"), "\\."))
  
  if(length(n_variables) > 1){
  best_model_n <- as.character(sort(n_variables[2]))
  }else{
    best_model_n <- as.character(n_variables[1])
  }
  
  best_train_model <- qs::qread(file.path(stage_1_path, glue("stage_1_rf_train_{flag}_{best_model_n}.qs")))
  
expanded_formula <- as.formula(
  paste("presence ~", paste(best_train_model$forest$independent.variable.names, collapse = " + "))
)

  rf_formula <- as.formula(deparse(formula(expanded_formula)) |> paste(collapse = " "))
  
  model_data_flag <- model_data %>%
    dplyr::select(lon, lat, flag_fin, length_category, year, presence) %>%
    filter(flag_fin == flag)
  
  
  distinct_cats <- model_data_flag %>%
    distinct(year, flag_fin, length_category)
  
  full_grid <- tidyr::crossing(env_grid, distinct_cats)
  
  data_random_forest <- full_grid %>%
    left_join(env_data, by = c("lon", "lat", "year")) %>%
    left_join(model_data_flag, by = c("lon", "lat", "year", "flag_fin", "length_category")) %>%
    mutate(presence = ifelse(is.na(presence), 0, presence)) %>%
    na.omit() %>%
    mutate(presence = as.factor(presence)) %>%
    distinct()
  
  
  set.seed(123)
  
  tic()
    model <- ranger(
        formula = rf_formula,
        data = data_random_forest,
        num.trees = 500, # run final with 500 trees
        importance = "impurity",
        classification = TRUE,
        probability = TRUE,
        write.forest = TRUE
      )
    
  toc()

  qs::qsave(model, glue(file.path(rdsi_dir, "prep/random_forest/artisanal_skylight_sentinel/stage_1_models/stage_1_rf_model_full_data_{flag}_{best_model_n}.qs")))

}

stopCluster(cl)
 

```


